---
layout: post
title: "深度学习笔试题笔记"
date: 2018-06-15
excerpt: "2018-06-15 深度学习、笔试 ."
tags: [深度学习, 笔试]
comments: true
---
# **深度学习笔试题（问答题）**

## 1. CNN的卷积核是单层还是多层？  
* &ensp;&ensp;一般而言，深度卷积神经网络是一层又一层的。层的本质是特征图，存储输入数据或者中间表达值。一组卷积核是联系前后两层的网络参数表达体，训练的目标就是每个卷积核中的参数。 
* &ensp;&ensp;描述网络中间层的厚度，一般称为feature map数或者通道数（channel）,一般称数据厚度为通道数，卷积核厚度为feature map数；卷积核一般是3维的，H\*W\*C,其中H\*W一般为卷积核的面积，C为卷积核的数量。若卷积核前一层厚度为1时，这一卷积为2D卷积，对应的平面点相乘后结果相加，相当于点积运算；若卷积核前一层厚度为>1时，这一卷积为3D卷积，分别对应每个平面点求卷积然后结果相加，相当于点积运算。  
* &ensp;&ensp;总之，卷积的意思就是把一个区域，不管是一维线段、二维方阵还是三维方块，全部按照卷积核的维度形状，对应的逐点相乘再求和，浓缩成一个标量值（降低到零维），然后作为下一层的一个点的值。 

## 2. 什么是卷积？  
* &ensp;&ensp;用滤波矩阵通过滑窗的方式对图像做內积（逐个元素相乘再求和）这样的操作就是卷积。  

## 3. 什么是CNN的池化层？  
* &ensp;&ensp;池化，就是用固定大小的窗口在特征图上按照一定大小的距离滑动，每个窗口取最大值（最大池化）或平均值（均值池化）来作为下一层的一个点的值。  

## 4. 简述一下什么是生成对抗式网络？  
* &ensp;&ensp;对抗样本：针对一个已经训练好的分类模型，将训练集中的一个样本做出一些细微的改变会导致模型给出一个错误的分类结果，这种虽然发生扰动但是人眼识别不出来，并且导致错误分类的样本称为对抗样本。  
* &ensp;&ensp;2014年GoodFellow提出了生成式对抗网络（Generative Adversarial Nets），它要解决的问题是如何从训练样本中学习出新的样本，训练样本是图片就生成新图片，训练样本是文章就生成新文章等等。如果能够知道训练样本的分布P(x),那么就可以在分布中随机采样得到新的样本。GAN则是在学习从随机变量z到训练样本x的映射关系，其中随机变量可以服从正太分布，那么就能得到一个由多层感知机组成的生成网络G(z,θg),网络的输入是一个一维的随机变量，输出的是一张图片，如何让输出的伪造图片看起来像训练样本，需要在生成网络后面接上一个多层感知机组成的判别网络D(x,θd),这个网络的出入是随机选择一张真实样本或者生成网络的输出，输出是输入图片来自真实样本P(data)或者生成网络P(G)的概率，当判别样本能够很好的分辨出输入的是不是真实样本时，也能通过梯度的方式说明什么样的输入更加像真实样本，从而通过这个信息来调整网络，从而G需要尽可能的让自己的输出更像真实样本，而D则是尽可能的将不是真实样本的情况判别出来。  
* &ensp;&ensp;GAN的优化是一个极大极小的博弈问题，最终的目的是G的输出给D时D很难判断是真实的还是伪造的，极大化D的判别能力，极小化G输出为伪造样本的能力。  
* &ensp;&ensp;训练GAN分为两个阶段：第一个阶段是训练D,冻结G;第二个阶段是训练G,冻结D.反复实现这两个操作，知道达到理想的效果。

## 5. 图片风格迁移的原理是什么？ 
* &ensp;&ensp;图像风格转换的原理是使用两张图片作为输入（A图作为内容输入（content input）,B图作为风格输入（style input））.最终目的是得到一张具有B风格和A内容的图片。
* &ensp;&ensp;利用卷积神经网络较低的特征图提取的特征来作为图片的内容重建，用较高维的特征来作为风格的重建。

## 5. 防止过拟合的方法有哪些？
&ensp;&ensp;过拟合（overfitting）是指模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合，使得模型在训练集上表现较好，在测试集上表现就很差，模型泛化能力弱。原因是数据少或者模型太复杂。
解决办法：
1. 获取更多的数据：这是最有效的方法，给模型足够多的数据，让模型学习尽可能多的例外情况，他就会不断修正自己，从而得到更好的结果。
    * 从数据源头获取更多数据
    * 数据增强
    * 根据当前数据集生成
2. 使用合适的模型：
    * 减少网络层数，神经元个数等均可以限制网络的拟合能力。
    * 训练时early stopping
    * 正则化：L1， L2(权重衰减：weight-decay)
    * 增加噪声：在输入中增加噪声，在权重上增加噪声，对网络响应增加噪声
3. 训练多种模型，取平均
    * Bagging
    * Boosting
    * Dropout(相当于权重正则化)
4. 贝叶斯方法

## 6. L1，L2是什么？什么情况下用L1,什么情况下用L2?
详见：[正则化](https://xmxxiong.github.io/正则化-post/)  
